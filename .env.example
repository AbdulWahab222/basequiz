# Ollama Configuration for AI-powered quiz generation
# Make sure Ollama is running locally (default: http://localhost:11434)
# Download Ollama from: https://ollama.ai

# Ollama server URL (default is localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434

# Ollama model to use for quiz generation
# Common models: llama3.2:3b, llama3.2, llama3:8b, mistral, etc.
OLLAMA_MODEL=llama3.2:3b

# Note: No API key needed - Ollama runs locally on your machine!
# Just make sure Ollama is running before starting the app.
