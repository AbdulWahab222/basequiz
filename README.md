# ğŸ§  AI Quiz Mini App

A Base Mini App that lets users generate and play quizzes on any topic. Built with Next.js, TypeScript, and styled with Tailwind CSS.

## âœ¨ Features

- **ğŸ¯ AI-Powered Quiz Generation**: Enter any topic and get a 5-question quiz generated by your local Ollama AI
- **ğŸ’¾ Local Storage**: Save quizzes to play later
- **ğŸ“¤ Share Results**: Share your quiz scores with friends using Web Share API
- **ğŸŒ™ Dark Mode Support**: Beautiful dark mode interface
- **ğŸ“± Responsive Design**: Works on all devices
- **âš¡ Fast & Modern**: Built with Next.js 14 and React 18
- **ğŸ”’ 100% Local**: No external API keys needed - uses your local Ollama installation

## ğŸš€ Quick Start

### Prerequisites

- Node.js 18+ installed
- npm or yarn package manager
- **Ollama** installed and running locally (see setup below)

### Ollama Setup (Required)

1. **Download and install Ollama:**
   - Visit [ollama.ai](https://ollama.ai) and download for Windows
   - Follow the installation instructions

2. **Start Ollama:**
   - Open a new terminal/command prompt
   - Run: `ollama serve`
   - Ollama will start on `http://localhost:11434`

3. **Pull the model (if not already done):**
   ```bash
   ollama pull llama3.2:3b
   ```
   - This downloads the ~2GB model file
   - Other models you can use: `llama3.2`, `llama3:8b`, `mistral`, `gemma:2b`

4. **Verify Ollama is running:**
   ```bash
   curl http://localhost:11434/api/tags
   ```
   - Should list your available models including llama3.2:3b

### Installation

1. **Install dependencies:**
```bash
npm install
```

2. **Configure Ollama (optional):**
   - Copy `.env.example` to `.env.local`:
     ```bash
     copy .env.example .env.local
     ```
   - Edit `.env.local` if you want to use a different model or URL

3. **Run the development server:**
```bash
npm run dev
```

4. **Open your browser:**
Navigate to [http://localhost:3000](http://localhost:3000)

## ğŸ“¦ Project Structure

```
quiz-mini-app/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ layout.tsx          # Root layout with metadata
â”‚   â”œâ”€â”€ page.tsx            # Main quiz application
â”‚   â””â”€â”€ globals.css         # Global styles
â”œâ”€â”€ public/
â”‚   â””â”€â”€ .well-known/
â”‚       â””â”€â”€ farcaster.json  # Farcaster manifest for Base Mini Apps
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â”œâ”€â”€ next.config.js
â””â”€â”€ README.md
```

## ğŸ® How to Use

1. **Enter a Topic**: Type any topic (e.g., "Cryptocurrency", "Space Exploration", "AI")
2. **Generate Quiz**: Click the "Generate Quiz" button
3. **Answer Questions**: Select answers for each of the 5 questions
4. **View Results**: See your score and review your answers
5. **Share**: Share your result with friends
6. **Save**: Save quizzes to play again later

## ğŸŒ Deploy to Vercel

### Option 1: Vercel CLI

1. **Install Vercel CLI:**
```bash
npm i -g vercel
```

2. **Deploy:**
```bash
vercel
```

### Option 2: Vercel Dashboard (Recommended)

1. **Push to GitHub:**
```bash
git init
git add .
git commit -m "Initial commit"
git branch -M main
git remote add origin https://github.com/YOUR_USERNAME/quiz-mini-app.git
git push -u origin main
```

2. **Deploy on Vercel:**
   - Go to [vercel.com](https://vercel.com)
   - Click "Add New Project"
   - Import your GitHub repository
   - Click "Deploy"

Your app will be live at `https://your-quiz-name.vercel.app`

## ğŸ”§ Configuration

### Environment Variables

The app uses Ollama for AI-powered quiz generation. Configure it in `.env.local`:

```bash
# Ollama server URL (default is localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434

# Ollama model to use for quiz generation
OLLAMA_MODEL=llama3.2:3b
```

### Available Ollama Models

You can use any model you have pulled with Ollama:
- `llama3.2:3b` - Fast, lightweight (recommended)
- `llama3.2` - Full 7B model
- `llama3:8b` - Llama 3 8B model
- `mistral` - Mistral 7B model
- `gemma:2b` - Google Gemma 2B model

To pull a new model:
```bash
ollama pull <model-name>
```

### Customize Quiz Generation

The quiz generation logic is in [`app/api/generate-quiz/route.ts`](app/api/generate-quiz/route.ts). You can customize:
- Number of questions (currently 5)
- Question difficulty level
- Response format
- Temperature and other generation parameters

## ğŸ“± Base Mini App Integration

This app includes the Farcaster manifest required for Base Mini Apps:

- **Farcaster Manifest**: Located at [`public/.well-known/farcaster.json`](public/.well-known/farcaster.json)
- **Open Graph Metadata**: Configured in [`app/layout.tsx`](app/layout.tsx)

## ğŸ¨ Styling

The app uses Tailwind CSS for styling. To customize:

1. **Colors**: Modify the gradient colors in [`app/page.tsx`](app/page.tsx)
2. **Layout**: Update the component structure
3. **Dark Mode**: Colors automatically adapt to system preferences

## ğŸ¤ Contributing

Contributions are welcome! Feel free to:
- Report bugs
- Suggest new features
- Submit pull requests

## ğŸ“„ License

MIT License - feel free to use this project for your own Base Mini App!

## ğŸ”— Resources

- [Base Mini Apps Documentation](https://docs.base.org/mini-apps)
- [Next.js Documentation](https://nextjs.org/docs)
- [React Documentation](https://react.dev)
- [Tailwind CSS Documentation](https://tailwindcss.com/docs)

## ğŸ†˜ Troubleshooting

### TypeScript Errors in VSCode

If you see TypeScript errors like:
- `Cannot find name 'process'`
- `Cannot find module 'react'`
- `Cannot find module 'next/server'`

**This is normal!** These errors appear because dependencies haven't been installed yet. They will be resolved after running `npm install`.

### Node.js/npm Not Found

If you see `'npm' is not recognized` or `'node' is not recognized`:

1. **Install Node.js:**
   - Download from [nodejs.org](https://nodejs.org)
   - Download the LTS version (recommended)
   - Run the installer and follow the prompts

2. **Verify installation:**
   ```bash
   node --version
   npm --version
   ```

3. **Restart your terminal/command prompt** after installation

### Build Errors

If you encounter build errors:
```bash
rm -rf .next node_modules
npm install
npm run build
```

### Port Already in Use

If port 3000 is busy:
```bash
npm run dev -- -p 3001
```

### Ollama Connection Issues

**Error: "Failed to generate quiz" or "Failed to connect to Ollama"**

1. **Check if Ollama is running:**
   ```bash
   curl http://localhost:11434/api/tags
   ```
   - If this fails, Ollama is not running
   - Start it with: `ollama serve`

2. **Verify the model is available:**
   ```bash
   ollama list
   ```
   - Should show `llama3.2:3b` or your configured model
   - If not, pull it: `ollama pull llama3.2:3b`

3. **Check your .env.local configuration:**
   - Ensure `OLLAMA_BASE_URL` matches your Ollama server
   - Default: `http://localhost:11434`

4. **First generation is slow:**
   - The first quiz generation may take 10-30 seconds
   - This is normal as the model loads into memory
   - Subsequent generations will be faster

### Quiz Generation Issues

**Questions are generic or not related to the topic:**

- Try a more specific topic (e.g., "Machine Learning" instead of "Computers")
- Adjust the temperature in [`app/api/generate-quiz/route.ts`](app/api/generate-quiz/route.ts)
- Try a different model (e.g., `llama3:8b` instead of `llama3.2:3b`)

**JSON parsing errors:**

- This can happen if Ollama returns malformed JSON
- The API includes fallback parsing logic
- Try again - sometimes the model needs a second attempt

## ğŸ“ Support

For issues or questions:
- Open an issue on GitHub
- Check the [Base Mini Apps Discord](https://discord.gg/base)

---

Built with â¤ï¸ for the Base ecosystem
