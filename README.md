# üß† AI Quiz Mini App

A Base Mini App that lets users generate and play quizzes on any topic. Built with Next.js, TypeScript, and styled with Tailwind CSS.

## ‚ú® Features

- **üéØ AI-Powered Quiz Generation**: Enter any topic and get a 5-question quiz generated by your local Ollama AI
- **üíæ Local Storage**: Save quizzes to play later
- **üì§ Share Results**: Share your quiz scores with friends using Web Share API
- **üåô Dark Mode Support**: Beautiful dark mode interface
- **üì± Responsive Design**: Works on all devices
- **‚ö° Fast & Modern**: Built with Next.js 14 and React 18
- **üîí 100% Local**: No external API keys needed - uses your local Ollama installation

## üöÄ Quick Start

### Prerequisites

- Node.js 18+ installed
- npm or yarn package manager
- **Ollama** installed and running locally (see setup below)

### Ollama Setup (Required)

1. **Download and install Ollama:**
   - Visit [ollama.ai](https://ollama.ai) and download for Windows
   - Follow the installation instructions

2. **Start Ollama:**
   - Open a new terminal/command prompt
   - Run: `ollama serve`
   - Ollama will start on `http://localhost:11434`

3. **Pull the model (if not already done):**
   ```bash
   ollama pull llama3.2:3b
   ```
   - This downloads the ~2GB model file
   - Other models you can use: `llama3.2`, `llama3:8b`, `mistral`, `gemma:2b`

4. **Verify Ollama is running:**
   ```bash
   curl http://localhost:11434/api/tags
   ```
   - Should list your available models including llama3.2:3b

### Installation

1. **Install dependencies:**
```bash
npm install
```

2. **Configure Ollama (optional):**
   - Copy `.env.example` to `.env.local`:
     ```bash
     copy .env.example .env.local
     ```
   - Edit `.env.local` if you want to use a different model or URL

3. **Run the development server:**
```bash
npm run dev
```

4. **Open your browser:**
Navigate to [http://localhost:3000](http://localhost:3000)

## üì¶ Project Structure

```
quiz-mini-app/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ layout.tsx          # Root layout with metadata
‚îÇ   ‚îú‚îÄ‚îÄ page.tsx            # Main quiz application
‚îÇ   ‚îî‚îÄ‚îÄ globals.css         # Global styles
‚îú‚îÄ‚îÄ public/
‚îÇ   ‚îî‚îÄ‚îÄ .well-known/
‚îÇ       ‚îî‚îÄ‚îÄ farcaster.json  # Farcaster manifest for Base Mini Apps
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ tsconfig.json
‚îú‚îÄ‚îÄ next.config.js
‚îî‚îÄ‚îÄ README.md
```

## üéÆ How to Use

1. **Enter a Topic**: Type any topic (e.g., "Cryptocurrency", "Space Exploration", "AI")
2. **Generate Quiz**: Click the "Generate Quiz" button
3. **Answer Questions**: Select answers for each of the 5 questions
4. **View Results**: See your score and review your answers
5. **Share**: Share your result with friends
6. **Save**: Save quizzes to play again later

## üåê Deploy to Vercel

### ‚ö†Ô∏è Important Note About Ollama Integration

**This app uses local Ollama for AI-powered quiz generation.** When deployed to Vercel, the app will **NOT** have access to your local Ollama instance because:

1. Vercel runs in the cloud, not on your local machine
2. The API route tries to connect to `localhost:11434`, which doesn't exist on Vercel
3. Vercel doesn't have Ollama installed

### Deployment Options

#### Option 1: Deploy as a Static Demo (Quiz Generation Disabled)

If you want to deploy to Vercel for demonstration purposes, the app will still work but quiz generation will fail. Users can only play with saved quizzes.

**Steps:**
1. **Push to GitHub** (already done!)
   - Repository: https://github.com/AbdulWahab222/basequiz

2. **Deploy on Vercel:**
   - Go to [vercel.com](https://vercel.com)
   - Click "Add New Project"
   - Import your GitHub repository
   - Click "Deploy"

Your app will be live at `https://basequiz.vercel.app`

#### Option 2: Deploy with a Cloud AI API (Full Functionality)

To enable quiz generation on Vercel, you need to use a cloud-based AI API instead of local Ollama:

1. **Get an API key** from one of these providers:
   - [OpenAI](https://platform.openai.com/api-keys) - GPT-4, GPT-3.5
   - [Anthropic](https://console.anthropic.com/) - Claude 3
   - [Groq](https://console.groq.com/) - Fast Llama/Mistral inference
   - [Together AI](https://api.together.xyz/) - Multiple open-source models

2. **Update the API route** in [`app/api/generate-quiz/route.ts`](app/api/generate-quiz/route.ts):
   - Replace the Ollama fetch call with your chosen API
   - Add your API key to environment variables

3. **Deploy to Vercel** with the new API integration

#### Option 3: Self-Host with Ollama (Full Functionality)

For full functionality with Ollama, you need to self-host the app:

1. **Deploy to a VPS** (e.g., DigitalOcean, AWS EC2, Railway)
2. **Install Ollama** on the server
3. **Run the app** with Ollama running alongside it
4. **Set up a reverse proxy** (nginx) for HTTPS

### Vercel CLI Deployment

If you still want to deploy to Vercel (for demo purposes):

```bash
npm i -g vercel
vercel
```

## üîß Configuration

### Environment Variables

The app uses Ollama for AI-powered quiz generation. Configure it in `.env.local`:

```bash
# Ollama server URL (default is localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434

# Ollama model to use for quiz generation
OLLAMA_MODEL=llama3.2:3b
```

### Available Ollama Models

You can use any model you have pulled with Ollama:
- `llama3.2:3b` - Fast, lightweight (recommended)
- `llama3.2` - Full 7B model
- `llama3:8b` - Llama 3 8B model
- `mistral` - Mistral 7B model
- `gemma:2b` - Google Gemma 2B model

To pull a new model:
```bash
ollama pull <model-name>
```

### Customize Quiz Generation

The quiz generation logic is in [`app/api/generate-quiz/route.ts`](app/api/generate-quiz/route.ts). You can customize:
- Number of questions (currently 5)
- Question difficulty level
- Response format
- Temperature and other generation parameters

## üì± Base Mini App Integration

This app includes the Farcaster manifest required for Base Mini Apps:

- **Farcaster Manifest**: Located at [`public/.well-known/farcaster.json`](public/.well-known/farcaster.json)
- **Open Graph Metadata**: Configured in [`app/layout.tsx`](app/layout.tsx)

## üé® Styling

The app uses Tailwind CSS for styling. To customize:

1. **Colors**: Modify the gradient colors in [`app/page.tsx`](app/page.tsx)
2. **Layout**: Update the component structure
3. **Dark Mode**: Colors automatically adapt to system preferences

## ü§ù Contributing

Contributions are welcome! Feel free to:
- Report bugs
- Suggest new features
- Submit pull requests

## üìÑ License

MIT License - feel free to use this project for your own Base Mini App!

## üîó Resources

- [Base Mini Apps Documentation](https://docs.base.org/mini-apps)
- [Next.js Documentation](https://nextjs.org/docs)
- [React Documentation](https://react.dev)
- [Tailwind CSS Documentation](https://tailwindcss.com/docs)

## üÜò Troubleshooting

### TypeScript Errors in VSCode

If you see TypeScript errors like:
- `Cannot find name 'process'`
- `Cannot find module 'react'`
- `Cannot find module 'next/server'`

**This is normal!** These errors appear because dependencies haven't been installed yet. They will be resolved after running `npm install`.

### Node.js/npm Not Found

If you see `'npm' is not recognized` or `'node' is not recognized`:

1. **Install Node.js:**
   - Download from [nodejs.org](https://nodejs.org)
   - Download the LTS version (recommended)
   - Run the installer and follow the prompts

2. **Verify installation:**
   ```bash
   node --version
   npm --version
   ```

3. **Restart your terminal/command prompt** after installation

### Build Errors

If you encounter build errors:
```bash
rm -rf .next node_modules
npm install
npm run build
```

### Port Already in Use

If port 3000 is busy:
```bash
npm run dev -- -p 3001
```

### Ollama Connection Issues

**Error: "Failed to generate quiz" or "Failed to connect to Ollama"**

1. **Check if Ollama is running:**
   ```bash
   curl http://localhost:11434/api/tags
   ```
   - If this fails, Ollama is not running
   - Start it with: `ollama serve`

2. **Verify the model is available:**
   ```bash
   ollama list
   ```
   - Should show `llama3.2:3b` or your configured model
   - If not, pull it: `ollama pull llama3.2:3b`

3. **Check your .env.local configuration:**
   - Ensure `OLLAMA_BASE_URL` matches your Ollama server
   - Default: `http://localhost:11434`

4. **First generation is slow:**
   - The first quiz generation may take 10-30 seconds
   - This is normal as the model loads into memory
   - Subsequent generations will be faster

### Quiz Generation Issues

**Questions are generic or not related to the topic:**

- Try a more specific topic (e.g., "Machine Learning" instead of "Computers")
- Adjust the temperature in [`app/api/generate-quiz/route.ts`](app/api/generate-quiz/route.ts)
- Try a different model (e.g., `llama3:8b` instead of `llama3.2:3b`)

**JSON parsing errors:**

- This can happen if Ollama returns malformed JSON
- The API includes fallback parsing logic
- Try again - sometimes the model needs a second attempt

## üìû Support

For issues or questions:
- Open an issue on GitHub
- Check the [Base Mini Apps Discord](https://discord.gg/base)

---

Built with ‚ù§Ô∏è for the Base ecosystem
